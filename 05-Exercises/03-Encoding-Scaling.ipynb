{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Machine Learning @ UDD\n",
    "### Instructor: Visiting Professor Rossano Schifanella"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representing Data and Engineering Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('retina')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-Hot-Encoding (Dummy variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# The file has no headers naming the columns, so we pass header=None\n",
    "# and provide the column names explicitly in \"names\"\n",
    "data = pd.read_csv(\"data/adult.csv\", header=None, index_col=False,\n",
    "    names=['age', 'workclass', 'fnlwgt', 'education',  'education-num',\n",
    "           'marital-status', 'occupation', 'relationship', 'race', 'gender',\n",
    "           'capital-gain', 'capital-loss', 'hours-per-week', 'native-country',\n",
    "           'income'])\n",
    "\n",
    "# For illustration purposes, we only select some of the columns:\n",
    "data = data[['age', 'workclass', 'education', 'gender', 'hours-per-week',\n",
    "             'occupation', 'income']]\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking string-encoded categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(data.gender.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.workclass.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data.workclass!=\"?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.workclass.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(data['income'])\n",
    "\n",
    "features = list(data.columns.values)\n",
    "features.remove('income')\n",
    "\n",
    "to_binarize = ['workclass', 'education', 'occupation', 'gender']\n",
    "\n",
    "for f in to_binarize:\n",
    "    data[f] = label_encoder.fit_transform(data[f])\n",
    "\n",
    "X = data[features].values\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OneHotEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoder = OneHotEncoder(sparse=False, categories='auto')\n",
    "X_encoded = one_hot_encoder.fit_transform(X)\n",
    "X_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's sometime easier to perform the OneHotEncoding using directly pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_encoded_pandas = pd.get_dummies(data[features], columns=['gender','workclass', 'education', 'occupation'])\n",
    "X_encoded_pandas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_encoded_pandas.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['hours-per-week'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['hours-per-week'].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "binarizer = Binarizer(threshold=39)\n",
    "binarizer.fit_transform(data['hours-per-week'].values.reshape(-1, 1))\n",
    "binary_hours_per_week= binarizer.fit_transform(data['hours-per-week'].values.reshape(-1, 1))\n",
    "\n",
    "for i in range(10):\n",
    "    print(data['hours-per-week'][i], binary_hours_per_week[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine, load_breast_cancer, load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_wine()\n",
    "# dataset = load_breast_cancer()\n",
    "# dataset = load_iris()\n",
    "\n",
    "X,y = dataset.data, dataset.target\n",
    "\n",
    "print(dataset.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(X, columns=dataset.feature_names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_corr_matrix(df):\n",
    "    \n",
    "    columns_names = df.columns\n",
    "    n_columns = len(columns_names)\n",
    "    \n",
    "    fig = plt.figure(figsize=(max(8, n_columns/3), max(8,n_columns/3)))\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(df.corr(), interpolation='nearest')\n",
    "    fig.colorbar(cax)\n",
    "    plt.xticks(np.arange(n_columns), columns_names, rotation=30, ha='left')\n",
    "    plt.yticks(range(n_columns), columns_names,)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_coxbox(X, columns_names, title=None):\n",
    "    n_columns = len(columns_names)\n",
    "    fig = plt.figure(figsize=(max(6, n_columns/3),max(6,n_columns/3)))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.boxplot(X)\n",
    "    plt.xticks(np.arange(1, n_columns + 1), columns_names, rotation=30, ha=\"right\")\n",
    "    if title:\n",
    "        plt.title(title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_corr_matrix(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_coxbox(X, dataset.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors=10)\n",
    "model.fit(X,y)\n",
    "\n",
    "print('average accuracy: %f' %np.average(cross_val_score(model, X, y, cv=10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of **standardization** (or **Z-score normalization**) is that the features will be rescaled so that they'll have the properties of a standard normal distribution with $\\mu = 0$ and $\\sigma = 1$\n",
    "\n",
    "where $\\mu$ is the mean (average) and $\\sigma$ is the standard deviation from the mean; standard scores (also called ***z*** scores) of the samples are calculated as follows:\n",
    "\n",
    "\\begin{equation} z = \\frac{x - \\mu}{\\sigma}\\end{equation} \n",
    "\n",
    "Standardizing the features so that they are centered around 0 with a standard deviation of 1 is not only important if we are comparing measurements that have different units, but it is also a general requirement for many machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MinMax Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative approach to Z-score normalization (or standardization) is the so-called **Min-Max scaling** (often also simply called \"normalization\" - a common cause for ambiguities).  \n",
    "In this approach, the data is scaled to a fixed range - usually 0 to 1.  \n",
    "The cost of having this bounded range - in contrast to standardization - is that we will end up with smaller standard deviations, which can suppress the effect of outliers.\n",
    "\n",
    "A Min-Max scaling is typically done via the following equation:\n",
    "\n",
    "\\begin{equation} X_{norm} = \\frac{X - X_{min}}{X_{max}-X_{min}} \\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_methods = [StandardScaler(), RobustScaler(), MinMaxScaler()]\n",
    "\n",
    "models = [DecisionTreeClassifier(random_state=43), \n",
    "          KNeighborsClassifier()]\n",
    "\n",
    "for model in models:\n",
    "    print(type(model).__name__)\n",
    "    reference_score = np.average(cross_val_score(model, X, y, cv=5))\n",
    "    \n",
    "    for scaler in scale_methods:\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        print('%s=> %f (%f)' %(type(scaler).__name__, \n",
    "                               np.average(cross_val_score(model, X_scaled, y, cv=5)), \n",
    "                               reference_score))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another visualization of the effect of scaling with synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, Normalizer\n",
    "\n",
    "X, y = make_blobs(n_samples=50, centers=2, random_state=4, cluster_std=1)\n",
    "X += 3 \n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "main_ax = plt.subplot2grid((2, 4), (0, 0), rowspan=2, colspan=2)\n",
    "\n",
    "main_ax.scatter(X[:, 0], X[:, 1], c=y, cmap=\"Accent\")\n",
    "maxx = np.abs(X[:, 0]).max()\n",
    "maxy = np.abs(X[:, 1]).max()\n",
    "\n",
    "main_ax.set_xlim(-maxx + 1, maxx + 1)\n",
    "main_ax.set_ylim(-maxy + 1, maxy + 1)\n",
    "main_ax.set_title(\"Original Data\")\n",
    "other_axes = [plt.subplot2grid((2, 4), (i, j))\n",
    "              for j in range(2, 4) for i in range(2)]\n",
    "\n",
    "for ax, scaler in zip(other_axes, [StandardScaler(), RobustScaler(),\n",
    "                                   MinMaxScaler(), Normalizer(norm='l2')]):\n",
    "    X_ = scaler.fit_transform(X)\n",
    "    ax.scatter(X_[:, 0], X_[:, 1], c=y, cmap=\"Accent\")\n",
    "    ax.set_xlim(-2, 2)\n",
    "    ax.set_ylim(-2, 2)\n",
    "    ax.set_title(type(scaler).__name__)\n",
    "\n",
    "other_axes.append(main_ax)\n",
    "\n",
    "for ax in other_axes:\n",
    "    ax.spines['left'].set_position('center')\n",
    "    ax.spines['right'].set_color('none')\n",
    "    ax.spines['bottom'].set_position('center')\n",
    "    ax.spines['top'].set_color('none')\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "    ax.yaxis.set_ticks_position('left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "\n",
    "For instance many elements used in the objective function of a learning algorithm (such as the RBF kernel of Support Vector Machines or the L1 and L2 regularizers of linear models) assume that all features are centered around 0 and have variance in the same order. If a feature has a variance that is orders of magnitude larger that others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some examples of algorithms where feature scaling matters are:\n",
    "\n",
    "- k-nearest neighbors with an Euclidean distance measure if want all features to contribute equally\n",
    "- k-means (see k-nearest neighbors)\n",
    "- logistic regression, SVMs, perceptrons, neural networks etc. if you are using gradient descent/ascent-based optimization, otherwise some weights will update much faster than others\n",
    "- linear discriminant analysis, principal component analysis, kernel principal component analysis since you want to find directions of maximizing the variance (under the constraints that those directions/eigenvectors/principal components are orthogonal); you want to have features on the same scale since you’d emphasize variables on “larger measurement scales” more. There are many more cases than I can possibly list here … I always recommend you to think about the algorithm and what it’s doing, and then it typically becomes obvious whether we want to scale your features or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Importance of Feature Scaling\n",
    "\n",
    "\n",
    "Feature scaling through standardization (or Z-score normalization)\n",
    "can be an important preprocessing step for many machine learning\n",
    "algorithms. Standardization involves rescaling the features such\n",
    "that they have the properties of a standard normal distribution\n",
    "with a mean of zero and a standard deviation of one.\n",
    "\n",
    "While many algorithms (such as SVM, K-nearest neighbors, and logistic\n",
    "regression) require features to be normalized, intuitively we can\n",
    "think of Principle Component Analysis (PCA) as being a prime example\n",
    "of when normalization is important. In PCA we are interested in the\n",
    "components that maximize the variance. If one component (e.g. human\n",
    "height) varies less than another (e.g. weight) because of their\n",
    "respective scales (meters vs. kilos), PCA might determine that the\n",
    "direction of maximal variance more closely corresponds with the\n",
    "'weight' axis, if those features are not scaled. As a change in\n",
    "height of one meter can be considered much more important than the\n",
    "change in weight of one kilogram, this is clearly incorrect.\n",
    "\n",
    "To illustrate this, PCA is performed comparing the use of data with\n",
    ":class:`StandardScaler <sklearn.preprocessing.StandardScaler>` applied,\n",
    "to unscaled data. The results are visualized and a clear difference noted.\n",
    "The 1st principal component in the unscaled set can be seen. It can be seen\n",
    "that feature #13 dominates the direction, being a whole two orders of\n",
    "magnitude above the other features. This is contrasted when observing\n",
    "the principal component for the scaled version of the data. In the scaled\n",
    "version, the orders of magnitude are roughly the same across all the features.\n",
    "\n",
    "The dataset used is the Wine Dataset available at UCI. This dataset\n",
    "has continuous features that are heterogeneous in scale due to differing\n",
    "properties that they measure (i.e alcohol content, and malic acid).\n",
    "\n",
    "The transformed data is then used to train a naive Bayes classifier, and a\n",
    "clear difference in prediction accuracies is observed wherein the dataset\n",
    "which is scaled before PCA vastly outperforms the unscaled version.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "FIG_SIZE = (10, 7)\n",
    "\n",
    "\n",
    "features, target = load_wine(return_X_y=True)\n",
    "\n",
    "# Make a train/test split using 30% test size\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target,\n",
    "                                                    test_size=0.30,\n",
    "                                                    random_state=RANDOM_STATE)\n",
    "\n",
    "# Fit to data and predict using pipelined GNB and PCA.\n",
    "unscaled_clf = make_pipeline(PCA(n_components=2), GaussianNB())\n",
    "unscaled_clf.fit(X_train, y_train)\n",
    "pred_test = unscaled_clf.predict(X_test)\n",
    "\n",
    "# Fit to data and predict using pipelined scaling, RandomForest and PCA.\n",
    "std_clf = make_pipeline(StandardScaler(), \n",
    "                        PCA(n_components=2), \n",
    "                        RandomForestClassifier(n_estimators=100))\n",
    "std_clf.fit(X_train, y_train)\n",
    "pred_test_std = std_clf.predict(X_test)\n",
    "\n",
    "# Show prediction accuracies in scaled and unscaled data.\n",
    "print('\\nPrediction accuracy for the normal test dataset with PCA')\n",
    "print('{:.2%}\\n'.format(metrics.accuracy_score(y_test, pred_test)))\n",
    "\n",
    "print('\\nPrediction accuracy for the standardized test dataset with PCA')\n",
    "print('{:.2%}\\n'.format(metrics.accuracy_score(y_test, pred_test_std)))\n",
    "\n",
    "# Extract PCA from pipeline\n",
    "pca = unscaled_clf.named_steps['pca']\n",
    "pca_std = std_clf.named_steps['pca']\n",
    "\n",
    "# Show first principal components\n",
    "print('\\nPC 1 without scaling:\\n', pca.components_[0])\n",
    "print('\\nPC 1 with scaling:\\n', pca_std.components_[0])\n",
    "\n",
    "# Scale and use PCA on X_train data for visualization.\n",
    "scaler = std_clf.named_steps['standardscaler']\n",
    "X_train_std = pca_std.transform(scaler.transform(X_train))\n",
    "\n",
    "# visualize standardized vs. untouched dataset with PCA performed\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=FIG_SIZE)\n",
    "\n",
    "\n",
    "for l, c, m in zip(range(0, 3), ('blue', 'red', 'green'), ('^', 's', 'o')):\n",
    "    ax1.scatter(X_train[y_train == l, 0], X_train[y_train == l, 1],\n",
    "                color=c,\n",
    "                label='class %s' % l,\n",
    "                alpha=0.5,\n",
    "                marker=m\n",
    "                )\n",
    "\n",
    "for l, c, m in zip(range(0, 3), ('blue', 'red', 'green'), ('^', 's', 'o')):\n",
    "    ax2.scatter(X_train_std[y_train == l, 0], X_train_std[y_train == l, 1],\n",
    "                color=c,\n",
    "                label='class %s' % l,\n",
    "                alpha=0.5,\n",
    "                marker=m\n",
    "                )\n",
    "\n",
    "ax1.set_title('Training dataset after PCA')\n",
    "ax2.set_title('Standardized training dataset after PCA')\n",
    "\n",
    "for ax in (ax1, ax2):\n",
    "    ax.set_xlabel('1st principal component')\n",
    "    ax.set_ylabel('2nd principal component')\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add polynomial features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often it's useful to add complexity to the model by considering nonlinear features of the input data. A simple and common method to use is polynomial features, which can get features’ high-order and interaction terms. It is implemented in the class ***<mark>PolynomialFeatures</mark>***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "np.random.seed(43)  #Setting seed for reproducability\n",
    "X = np.zeros(shape=(60,1))\n",
    "X[:,0] = np.array([i*np.pi/180 for i in range(60,360,5)])\n",
    "\n",
    "y = np.sin(X[:,0]) + np.random.normal(0,0.15,len(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[:,0], y, c=\"gray\", edgecolors=(0, 0, 0))\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add polynomial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_features = PolynomialFeatures(degree=5)\n",
    "\n",
    "polynomial_features.fit(X)\n",
    "X_polynomial = polynomial_features.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_features.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_polynomial.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a linear regression with the new polynomial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linear_regression = LinearRegression().fit(X_polynomial, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define points to be fit in the polynomial linear regression\n",
    "line = np.linspace(X.min(), X.max(), 1000)[:-1].reshape(-1, 1)\n",
    "line_poly = polynomial_features.transform(line)\n",
    "\n",
    "plt.plot(X[:, 0], y, 'o')\n",
    "plt.plot(line, linear_regression.predict(line_poly), label='polynomial linear regression')\n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: play with the degree of the polynomial function and see the effect in the fit. Could you hypothesize the presence of overfitting?   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
